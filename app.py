"""
SCM Dashboard - Streamlit ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
"""
import streamlit as st
import pandas as pd
from scm.config import config
from scm.io.excel import load_from_excel
from scm.io.sheets import load_from_gsheet_api, load_snapshot_raw_from_gsheet
from scm.transform.normalize import normalize_moves, normalize_refined_snapshot
from scm.transform.wip import load_wip_from_incoming, merge_wip_as_moves
from scm.domain.timeline import build_timeline
from scm.domain.forecast import apply_consumption_with_events
from scm.domain.cost import pivot_inventory_cost_from_raw
from scm.utils.dates import today, get_date_range, clamp_date_range

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ê¸€ë¡œë²Œ ëŒ€ì‹œë³´ë“œ â€” v5", layout="wide")
st.title("ğŸ“¦ SCM ì¬ê³  íë¦„ ëŒ€ì‹œë³´ë“œ â€” v5 (ëª¨ë“ˆí™”)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "_data_source" not in st.session_state:
    st.session_state["_data_source"] = None
if "_snapshot_raw_cache" not in st.session_state:
    st.session_state["_snapshot_raw_cache"] = None

# ìºì‹œëœ í•¨ìˆ˜ë“¤
@st.cache_data(ttl=config.CACHE_TTL_EXCEL)
def _load_excel_cached(file):
    """Excel ë¡œë”© ìºì‹œ ë˜í¼"""
    return load_from_excel(file)

@st.cache_data(ttl=config.CACHE_TTL_GSHEET)
def _load_gsheet_cached():
    """Google Sheets ë¡œë”© ìºì‹œ ë˜í¼"""
    try:
        gs = st.secrets["google_sheets"]
        creds_obj = gs.get("credentials", None)
        creds_json = gs.get("credentials_json", None)
        
        if creds_obj is not None:
            if isinstance(creds_obj, dict):
                credentials_info = dict(creds_obj)
            else:
                credentials_info = {k: creds_obj[k] for k in creds_obj.keys()}
        elif creds_json:
            import json
            credentials_info = json.loads(str(creds_json))
        else:
            raise ValueError("Google Sheets ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        return load_from_gsheet_api(config.GSHEET_ID, credentials_info)
    except Exception as e:
        st.error(f"Google Sheets ë¡œë”© ì‹¤íŒ¨: {e}")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

@st.cache_data(ttl=config.CACHE_TTL_EVENTS)
def _apply_consumption_cached(timeline, snap_long, centers_sel, skus_sel, start_dt, end_dt, lookback_days, events):
    """ì†Œì§„ ì˜ˆì¸¡ ìºì‹œ ë˜í¼"""
    return apply_consumption_with_events(
        timeline, snap_long, centers_sel, skus_sel,
        start_dt, end_dt, lookback_days, events
    )

# ë°ì´í„° ë¡œë”© UI
tab1, tab2 = st.tabs(["ì—‘ì…€ ì—…ë¡œë“œ", "Google Sheets"])

with tab1:
    xfile = st.file_uploader("ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], key="excel")
    if xfile is not None:
        try:
            df_move, df_ref, df_incoming, snap_raw_df = _load_excel_cached(xfile)
            st.session_state["_data_source"] = "excel"
            st.session_state["_snapshot_raw_cache"] = snap_raw_df
            
            moves_raw = normalize_moves(df_move)
            snap_long = normalize_refined_snapshot(df_ref)
            
            try:
                wip_df = load_wip_from_incoming(df_incoming)
                moves = merge_wip_as_moves(moves_raw, wip_df)
                st.success(f"WIP {len(wip_df)}ê±´ ë°˜ì˜ ì™„ë£Œ" if wip_df is not None and not wip_df.empty else "WIP ì—†ìŒ")
            except Exception as e:
                moves = moves_raw
                st.warning(f"WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            st.error(f"Excel ë¡œë”© ì‹¤íŒ¨: {e}")

with tab2:
    st.info("Google Sheets APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    
    if st.button("Google Sheetsì—ì„œ ë°ì´í„° ë¡œë“œ", type="primary"):
        try:
            df_move, df_ref, df_incoming = _load_gsheet_cached()
            
            if df_move.empty or df_ref.empty:
                st.error("âŒ Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.session_state["_data_source"] = "gsheet"
                
                moves_raw = normalize_moves(df_move)
                snap_long = normalize_refined_snapshot(df_ref)
                
                try:
                    wip_df = load_wip_from_incoming(df_incoming)
                    moves = merge_wip_as_moves(moves_raw, wip_df)
                    st.success(f"âœ… Google Sheets ë¡œë“œ ì™„ë£Œ! WIP {len(wip_df)}ê±´ ë°˜ì˜" if wip_df is not None and not wip_df.empty else "âœ… Google Sheets ë¡œë“œ ì™„ë£Œ! WIP ì—†ìŒ")
                except Exception as e:
                    moves = moves_raw
                    st.warning(f"âš ï¸ WIP ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        except Exception as e:
            st.error(f"âŒ Google Sheets ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

# ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ì•ˆë‚´
if "snap_long" not in locals():
    st.info("ì—‘ì…€ ì—…ë¡œë“œ ë˜ëŠ” Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ í•„í„°/ì°¨íŠ¸ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    st.stop()

# í•„í„° UI
st.sidebar.header("í•„í„°")

# ì„¼í„° ë° SKU ì„ íƒ
centers_snap = set(snap_long["center"].dropna().astype(str).unique().tolist())
centers_moves = set(moves["from_center"].dropna().astype(str).unique().tolist() + 
                   moves["to_center"].dropna().astype(str).unique().tolist())

def normalize_center_name(center):
    if center in ["", "nan", "None", "WIP", "In-Transit"]:
        return None
    if center in ["AcrossBUS", "ì–´í¬ë¡œìŠ¤ë¹„US"]:
        return "ì–´í¬ë¡œìŠ¤ë¹„US"
    return center

all_centers = set()
for center in centers_snap | centers_moves:
    normalized = normalize_center_name(center)
    if normalized:
        all_centers.add(normalized)

centers = sorted(list(all_centers))
skus = sorted(snap_long["resource_code"].dropna().astype(str).unique().tolist())

centers_sel = st.sidebar.multiselect("ì„¼í„° ì„ íƒ", centers, default=(["íƒœê´‘KR"] if "íƒœê´‘KR" in centers else centers[:1]))
skus_sel = st.sidebar.multiselect("SKU ì„ íƒ", skus, default=([s for s in ["BA00022","BA00021"] if s in skus] or skus[:2]))

# ê¸°ê°„ ì„¤ì •
_today = today()
start_dt, end_dt = get_date_range(_today, config.PAST_DAYS, config.FUTURE_DAYS)

st.sidebar.subheader("ê¸°ê°„ ì„¤ì •")
horizon_days = st.sidebar.number_input("ë¯¸ë˜ ì „ë§ ì¼ìˆ˜", min_value=0, max_value=config.FUTURE_DAYS, step=1, value=20)

date_range = st.sidebar.date_input("ê¸°ê°„",
    value=(_today - pd.Timedelta(days=20), _today + pd.Timedelta(days=20)),
    min_value=start_dt.date(),
    max_value=end_dt.date(),
    format="YYYY-MM-DD")

start_dt = pd.Timestamp(date_range[0]).normalize()
end_dt = pd.Timestamp(date_range[1]).normalize()

# í‘œì‹œ ì˜µì…˜
st.sidebar.header("í‘œì‹œ ì˜µì…˜")
show_prod = st.sidebar.checkbox("ìƒì‚°ì¤‘(ë¯¸ì™„ë£Œ) í‘œì‹œ", value=True)
show_transit = st.sidebar.checkbox("ì´ë™ì¤‘ í‘œì‹œ", value=True)
use_cons_forecast = st.sidebar.checkbox("ì¶”ì„¸ ê¸°ë°˜ ì¬ê³  ì˜ˆì¸¡", value=True)
lookback_days = st.sidebar.number_input("ì¶”ì„¸ ê³„ì‚° ê¸°ê°„(ì¼)", min_value=7, max_value=56, value=config.DEFAULT_LOOKBACK_DAYS, step=7)

# ì…ê³  ë°˜ì˜ ê°€ì • ì˜µì…˜
st.sidebar.subheader("ì…ê³  ë°˜ì˜ ê°€ì •")
lag_days = st.sidebar.number_input("ì…ê³  ë°˜ì˜ ë¦¬ë“œíƒ€ì„(ì¼) â€“ inbound ë¯¸ê¸°ë¡ ì‹œ arrival+N", 
                                   min_value=0, max_value=21, value=config.ARRIVAL_TO_INBOUND_LAG_DAYS, step=1)

# ì´ë²¤íŠ¸ ì„¤ì •
with st.sidebar.expander("í”„ë¡œëª¨ì…˜ ê°€ì¤‘ì¹˜(+%)", expanded=False):
    enable_event = st.checkbox("ê°€ì¤‘ì¹˜ ì ìš©", value=False)
    ev_start = st.date_input("ì‹œì‘ì¼")
    ev_end   = st.date_input("ì¢…ë£Œì¼")
    ev_pct   = st.number_input("ê°€ì¤‘ì¹˜(%)", min_value=-100.0, max_value=300.0, value=30.0, step=5.0)
events = [{"start": pd.Timestamp(ev_start).strftime("%Y-%m-%d"),
           "end":   pd.Timestamp(ev_end).strftime("%Y-%m-%d"),
           "uplift": ev_pct/100.0}] if enable_event else []

# ë©”ì¸ ë¡œì§
if centers_sel and skus_sel:
    # íƒ€ì„ë¼ì¸ êµ¬ì¶•
    _latest_snap = snap_long["date"].max()
    proj_days_for_build = max(0, int((end_dt - _latest_snap).days))
    
    timeline = build_timeline(snap_long, moves, centers_sel, skus_sel,
                              start_dt, end_dt, horizon_days=proj_days_for_build, today=_today,
                              lag_days=int(lag_days))
    
    # ì†Œì§„ ì˜ˆì¸¡ ì ìš©
    if use_cons_forecast and not timeline.empty:
        timeline = _apply_consumption_cached(
            timeline, snap_long, centers_sel, skus_sel,
            start_dt, end_dt, lookback_days=int(lookback_days), events=events
        )
    
    # ì˜¤ëŠ˜ ì•µì»¤: ì°¨íŠ¸ì˜ 'ì˜¤ëŠ˜' ê°’ì„ ìŠ¤ëƒ…ìƒ· ê°’ìœ¼ë¡œ ê³ ì •
    latest_dt = snap_long["date"].max()
    anchor_base = (snap_long[
        (snap_long["date"] == latest_dt) &
        (snap_long["center"].isin(centers_sel)) &
        (snap_long["resource_code"].isin(skus_sel))
    ].groupby(["center","resource_code"])["stock_qty"].sum())
    
    for (ct, sku), y in anchor_base.items():
        m = ((timeline["center"] == ct) &
             (timeline["resource_code"] == sku) &
             (timeline["date"] == _today))
        timeline.loc[m, "stock_qty"] = int(y)
    
    # ê²°ê³¼ í‘œì‹œ
    if timeline.empty:
        st.info("ì„ íƒ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” íƒ€ì„ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.success(f"âœ… íƒ€ì„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ! {len(timeline)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
        st.dataframe(timeline.head(10))
        
        # ê°„ë‹¨í•œ í†µê³„
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ë°ì´í„° í¬ì¸íŠ¸", f"{len(timeline):,}")
        with col2:
            st.metric("ì„¼í„° ìˆ˜", f"{timeline['center'].nunique()}")
        with col3:
            st.metric("SKU ìˆ˜", f"{timeline['resource_code'].nunique()}")
else:
    st.warning("ì„¼í„°ì™€ SKUë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
